{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33343b9a",
   "metadata": {},
   "source": [
    "# TABULAR DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07409bb6",
   "metadata": {},
   "source": [
    "## 1. Imports and settings\n",
    "\n",
    "Import required libraries and configure display options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aad895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91b4a5",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n",
    "\n",
    "Load a CSV file into a DataFrame. Change DATA_PATH to your file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1417504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tabular_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3cfcf",
   "metadata": {},
   "source": [
    "## 3. Initial inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ac3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21093bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7728e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"col\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"col\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c36afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce583aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e272ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d465be",
   "metadata": {},
   "source": [
    "## PreProcess Date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53fb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datetime(df, datetime_column, format=None):\n",
    "    \"\"\"Convert datetime column to datetime type and set as index.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[datetime_column] = pd.to_datetime(df[datetime_column], format=format, errors='coerce')\n",
    "    df.set_index(datetime_column, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# df = parse_datetime(df, 'timestamp', format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9545e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df):\n",
    "    \"\"\"Extract time-based features from datetime index.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['hour'] = df.index.hour\n",
    "    df['weekday'] = df.index.weekday\n",
    "    df['is_weekend'] = df.index.weekday >= 5\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd069fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6364bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_time_series(df, numeric_columns, method='ffill', n_neighbors=3):\n",
    "    \"\"\"Impute missing values in time series data.\"\"\"\n",
    "    df = df.copy()\n",
    "    if method in ['ffill', 'bfill']:\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(method=method)\n",
    "    elif method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# df = impute_time_series(df, numeric_columns=['value'], method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21427455",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize numerical and categorical distributions, boxplots for outliers, and correlation heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37174b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d424498",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "categorical_columns = df.select_dtypes(include=['object','category','bool']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21780edd",
   "metadata": {},
   "source": [
    "### 4.1 Numerical features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c67183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distributions(df, num_cols, bins=50):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=bins)\n",
    "        plt.title(f'Distribution: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003d7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_numeric_distributions(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb98b1a",
   "metadata": {},
   "source": [
    "### 4.2 Handling Skewed Numerical Features\n",
    "\n",
    "To handle skewed distributions in numerical features, you can add a section that checks the skewness and applies transformations if necessary. Skewness is typically considered significant if it's greater than 1 (right-skewed) or less than -1 (left-skewed). Common transformations include:\n",
    "\n",
    "- **Log Transformation** (useful for right-skewed data with positive values): `np.log1p(df[col])` to handle zeros.\n",
    "- **Square Root Transformation** (for moderate right-skew): `np.sqrt(df[col])`.\n",
    "- **Box-Cox Transformation** (requires positive values; handles both skew directions): From `scipy.stats.boxcox`.\n",
    "- **Yeo-Johnson Transformation** (handles negative/zero values): From `sklearn.preprocessing.PowerTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74344857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import boxcox, skew\n",
    "# from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267af8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log_transformation(df, num_cols, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Applies log transformation to skewed numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - num_cols: List of numerical column names.\n",
    "    - skew_threshold: Threshold for considering a distribution skewed (default: 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()\n",
    "    for col in num_cols:\n",
    "        col_skew = skew(transformed_df[col].dropna())\n",
    "        print(f\"Skewness of {col}: {col_skew:.2f}\")\n",
    "        \n",
    "        if abs(col_skew) > skew_threshold:\n",
    "            if (transformed_df[col] < 0).any():\n",
    "                print(f\"Warning: {col} has negative values; skipping log transform.\")\n",
    "                continue\n",
    "            print(f\"Applying log transformation to {col}...\")\n",
    "            transformed_df[col] = np.log1p(transformed_df[col])\n",
    "            \n",
    "            # Recheck skewness\n",
    "            new_skew = skew(transformed_df[col].dropna())\n",
    "            print(f\"New skewness of {col}: {new_skew:.2f}\")\n",
    "        \n",
    "        # Plot before and after\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
    "        axes[0].set_title(f'Original Distribution: {col}')\n",
    "        sns.histplot(transformed_df[col].dropna(), kde=True, ax=axes[1])\n",
    "        axes[1].set_title(f'Log Transformed Distribution: {col}')\n",
    "        plt.show()\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sqrt_transformation(df, num_cols, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Applies square root transformation to skewed numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - num_cols: List of numerical column names.\n",
    "    - skew_threshold: Threshold for considering a distribution skewed (default: 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()\n",
    "    for col in num_cols:\n",
    "        col_skew = skew(transformed_df[col].dropna())\n",
    "        print(f\"Skewness of {col}: {col_skew:.2f}\")\n",
    "        \n",
    "        if abs(col_skew) > skew_threshold:\n",
    "            if (transformed_df[col] < 0).any():\n",
    "                print(f\"Warning: {col} has negative values; skipping sqrt transform.\")\n",
    "                continue\n",
    "            print(f\"Applying square root transformation to {col}...\")\n",
    "            transformed_df[col] = np.sqrt(transformed_df[col])\n",
    "            \n",
    "            # Recheck skewness\n",
    "            new_skew = skew(transformed_df[col].dropna())\n",
    "            print(f\"New skewness of {col}: {new_skew:.2f}\")\n",
    "        \n",
    "        # Plot before and after\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
    "        axes[0].set_title(f'Original Distribution: {col}')\n",
    "        sns.histplot(transformed_df[col].dropna(), kde=True, ax=axes[1])\n",
    "        axes[1].set_title(f'Square Root Transformed Distribution: {col}')\n",
    "        plt.show()\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_boxcox_transformation(df, num_cols, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Applies Box-Cox transformation to skewed numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - num_cols: List of numerical column names.\n",
    "    - skew_threshold: Threshold for considering a distribution skewed (default: 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()\n",
    "    for col in num_cols:\n",
    "        col_skew = skew(transformed_df[col].dropna())\n",
    "        print(f\"Skewness of {col}: {col_skew:.2f}\")\n",
    "        \n",
    "        if abs(col_skew) > skew_threshold:\n",
    "            if (transformed_df[col] <= 0).any():\n",
    "                print(f\"Warning: {col} has non-positive values; skipping Box-Cox.\")\n",
    "                continue\n",
    "            print(f\"Applying Box-Cox transformation to {col}...\")\n",
    "            transformed_df[col], _ = boxcox(transformed_df[col] + 1e-6)  # Small epsilon for stability\n",
    "            \n",
    "            # Recheck skewness\n",
    "            new_skew = skew(transformed_df[col].dropna())\n",
    "            print(f\"New skewness of {col}: {new_skew:.2f}\")\n",
    "        \n",
    "        # Plot before and after\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
    "        axes[0].set_title(f'Original Distribution: {col}')\n",
    "        sns.histplot(transformed_df[col].dropna(), kde=True, ax=axes[1])\n",
    "        axes[1].set_title(f'Box-Cox Transformed Distribution: {col}')\n",
    "        plt.show()\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_yeojohnson_transformation(df, num_cols, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Applies Yeo-Johnson transformation to skewed numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - num_cols: List of numerical column names.\n",
    "    - skew_threshold: Threshold for considering a distribution skewed (default: 1.0).\n",
    "    \n",
    "    Returns:\n",
    "    - Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()\n",
    "    for col in num_cols:\n",
    "        col_skew = skew(transformed_df[col].dropna())\n",
    "        print(f\"Skewness of {col}: {col_skew:.2f}\")\n",
    "        \n",
    "        if abs(col_skew) > skew_threshold:\n",
    "            print(f\"Applying Yeo-Johnson transformation to {col}...\")\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            transformed_df[col] = pt.fit_transform(transformed_df[[col]]).flatten()\n",
    "            \n",
    "            # Recheck skewness\n",
    "            new_skew = skew(transformed_df[col].dropna())\n",
    "            print(f\"New skewness of {col}: {new_skew:.2f}\")\n",
    "        \n",
    "        # Plot before and after\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
    "        axes[0].set_title(f'Original Distribution: {col}')\n",
    "        sns.histplot(transformed_df[col].dropna(), kde=True, ax=axes[1])\n",
    "        axes[1].set_title(f'Yeo-Johnson Transformed Distribution: {col}')\n",
    "        plt.show()\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc8a27",
   "metadata": {},
   "source": [
    "### 4.3 Categorical features count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_counts(df, cat_cols, top_n=20):\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.countplot(y=df[col], order=df[col].value_counts().index[:top_n])\n",
    "        plt.title(f'Counts: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024df5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_categorical_counts(df, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4eef1",
   "metadata": {},
   "source": [
    "### 4.4 Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df, num_cols):\n",
    "    corr = df[num_cols].corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_heatmap(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb0b54",
   "metadata": {},
   "source": [
    "### 4.5 Boxplot to check outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c97659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, num_cols):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(6,2))\n",
    "        sns.boxplot(x=df[col].dropna())\n",
    "        plt.title(f'Boxplot: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e744e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_boxplots(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f25152",
   "metadata": {},
   "source": [
    "## 5. Missing Value Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7b0fd",
   "metadata": {},
   "source": [
    "### 5.1 Detecting Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def msno_show (df) :\n",
    "    msno.matrix(df)\n",
    "    plt.show()\n",
    "\n",
    "    msno.bar(df)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#msno_show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a340080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hidden_missing_data(df, suspicious_values=None, target_column=None):\n",
    "    \"\"\"\n",
    "    Detects missing data in the DataFrame for all columns or a specific column, \n",
    "    including standard NaN/None and custom suspicious values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - suspicious_values: List of custom values to treat as missing (e.g., [-999, 'unknown', '']). \n",
    "                         Defaults to None (only checks NaN/None).\n",
    "    - target_column: String, name of a specific column to check for missing data. \n",
    "                    If None, checks all columns. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame summarizing missing counts and percentages for the selected column(s).\n",
    "    \"\"\"\n",
    "    if suspicious_values is None:\n",
    "        suspicious_values = []\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    # Validate target_column\n",
    "    if target_column is not None:\n",
    "        if target_column not in temp_df.columns:\n",
    "            print(f\"Error: Column '{target_column}' not found in DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        temp_df = temp_df[[target_column]]  # Focus on single column\n",
    "        print(f\"Checking missing data for column: {target_column}\")\n",
    "    else:\n",
    "        print(\"Checking missing data for all columns\")\n",
    "    \n",
    "    # Replace suspicious values with NaN\n",
    "    for val in suspicious_values:\n",
    "        temp_df = temp_df.replace(val, np.nan)\n",
    "    \n",
    "    # Calculate missing counts and percentages\n",
    "    missing_counts = temp_df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(temp_df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing Percentage (%)': missing_percentages.round(2)\n",
    "    }).sort_values(by='Missing Count', ascending=False)\n",
    "    \n",
    "    # Filter summary to show only columns with missing data\n",
    "    missing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n",
    "    \n",
    "    # Print summary\n",
    "    if not missing_summary.empty:\n",
    "        print(\"Missing Data Summary:\")\n",
    "        print(missing_summary)\n",
    "    else:\n",
    "        print(\"No missing data detected in the selected column(s).\")\n",
    "    \n",
    "    # Visualize missing data with heatmap (only if there are missing values)\n",
    "    if missing_counts.sum() > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(temp_df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "        plt.title(f'Missing Data Heatmap {\"for \" + target_column if target_column else \"for All Columns\"}')\n",
    "        plt.show()\n",
    "    \n",
    "    return missing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae59360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is your DataFrame\n",
    "# suspicious = [-999, 'unknown', '', 'N/A']  # Customize based on your data\n",
    "\n",
    "# # Check for a specific column\n",
    "# missing_summary = detect_hidden_missing_data(df, suspicious_values=suspicious, target_column='column_name')\n",
    "\n",
    "# # Check for all columns\n",
    "# missing_summary_all = detect_hidden_missing_data(df, suspicious_values=suspicious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87a40d",
   "metadata": {},
   "source": [
    "### 5.2 Drop All Missing Row (in every feutures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(df):\n",
    "    \"\"\"\n",
    "    Remove rows with missing values.\n",
    "    \"\"\"\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a146aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = drop_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4345465",
   "metadata": {},
   "source": [
    "### 5.3 Drop All Missing Row (in Specific feutures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['column_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7506e",
   "metadata": {},
   "source": [
    "### 5.4 Drop Columns\n",
    "Use drop_columns when you want to drop entire useless columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    \"\"\"Drop specified columns from the DataFrame.\"\"\"\n",
    "    return df.drop(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ed56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_columns(df, ['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a052b1d",
   "metadata": {},
   "source": [
    "### 5.5 Drop Rows\n",
    "Use drop_rows when the percentage of missing data is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3efb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df, rows):\n",
    "    \"\"\"Drop specified rows from the DataFrame.\"\"\"\n",
    "    return df.drop(index=rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = drop_rows(df , ['row_index1' , 'row_index_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5251d5",
   "metadata": {},
   "source": [
    "### 5.6 Statistical Imputation\n",
    "Use fill_with_statistical for numerical features when distribution is stable.\n",
    "> Note: Before use **statistical imputation** split your Dataset, because of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_statistical(df, num_col, strategy=\"mean\"):\n",
    "    \"\"\"Fill missing with mean, median or mode.\"\"\"\n",
    "    if strategy == \"mean\":\n",
    "        df[num_col] = df[num_col].fillna(df[num_col].mean())\n",
    "    elif strategy == \"median\":\n",
    "        df[num_col] = df[num_col].fillna(df[num_col].median())\n",
    "    elif strategy == \"mode\":\n",
    "        df[num_col] = df[num_col].fillna(df[num_col].mode().iloc[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_with_statistical(df, columns=['num_col1', 'num_col2'], method=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6c7f1",
   "metadata": {},
   "source": [
    "### 5.7 Categorical Imputation\n",
    "Use fill_categorical for categorical features.\n",
    "> Note: Before use **categorical imputation** split your Dataset, because of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5843c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical(df, cat_cols):\n",
    "    \"\"\"Fill missing categorical values with mode.\"\"\"\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1dc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_categorical(df, cat_cols=['cat_col1', 'cat_col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407b432",
   "metadata": {},
   "source": [
    "### 5.8 Forward and Backward Fill\n",
    "Use interpolation for time-series or continuous numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_ffill_bfill(df, columns, method=\"ffill\"):\n",
    "    \"\"\"Fill using forward fill or backward fill.\"\"\"\n",
    "    df[columns] = df[columns].fillna(method=method)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7681cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_with_ffill_bfill(df, columns=['num_col1', 'num_col2'], method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf00cbf",
   "metadata": {},
   "source": [
    "### 5.9 Imputation Techniques\n",
    "Use KNN imputation when you expect relationships between features.\n",
    "> Note: Before use **KNN imputation** split your Dataset, because of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2890ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def fill_with_knn(df, numeric_columns, n_neighbors=3):\n",
    "    \"\"\"Impute missing values using KNN.\"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_with_knn(df, numeric_columns=['num_col1', 'num_col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_iterative_imputer(df, numeric_columns, estimator=LinearRegression(), max_iter=10, random_state=42, verbose=False):\n",
    "    \"\"\"\n",
    "    Impute missing values in numerical columns using IterativeImputer.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - numeric_columns: List of numerical column names to impute.\n",
    "    - estimator: Estimator for imputation (default: LinearRegression()).\n",
    "    - max_iter: Maximum number of imputation iterations (default: 10).\n",
    "    - random_state: Random seed for reproducibility (default: 42).\n",
    "    - verbose: If True, prints imputation statistics (default: False).\n",
    "    \n",
    "    Returns:\n",
    "    - Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    imputed_df = df.copy()\n",
    "    \n",
    "    # Count missing values before imputation\n",
    "    missing_before = imputed_df[numeric_columns].isnull().sum()\n",
    "    \n",
    "    # Initialize and fit the imputer\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=estimator,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        skip_complete=True  # Skip columns with no missing values\n",
    "    )\n",
    "    imputed_df[numeric_columns] = imputer.fit_transform(imputed_df[numeric_columns])\n",
    "    \n",
    "    # Count missing values after imputation\n",
    "    missing_after = imputed_df[numeric_columns].isnull().sum()\n",
    "    \n",
    "    # Print imputation stats if verbose\n",
    "    if verbose:\n",
    "        print(\"Imputation Summary:\")\n",
    "        for col in numeric_columns:\n",
    "            if missing_before[col] > 0:\n",
    "                print(f\"{col}: Imputed {missing_before[col]} missing values\")\n",
    "        if missing_after.sum() == 0:\n",
    "            print(\"All missing values successfully imputed.\")\n",
    "        else:\n",
    "            print(\"Warning: Some missing values remain after imputation.\")\n",
    "    \n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f514c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'df' is your DataFrame and 'numeric_columns' is your list of numerical columns\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Impute with default settings\n",
    "# imputed_df = fill_with_iterative_imputer(df, numeric_columns, verbose=True)\n",
    "\n",
    "# # Impute with a different estimator (e.g., RandomForestRegressor)\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# imputed_df_rf = fill_with_iterative_imputer(\n",
    "#     df, \n",
    "#     numeric_columns, \n",
    "#     estimator=RandomForestRegressor(n_estimators=50, random_state=42), \n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c913a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def detect_and_impute_missing(df, numerical_strategy='mean', categorical_strategy='most_frequent'):\n",
    "    \"\"\"Detect categorical/numerical columns and impute missing values.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Detect column types\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"Numerical Columns: {numerical_cols}\")\n",
    "    \n",
    "    # Impute categorical columns\n",
    "    cat_imputer = None\n",
    "    if categorical_cols:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    # Impute numerical columns\n",
    "    num_imputer = None\n",
    "    if numerical_cols:\n",
    "        num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "        df[numerical_cols] = num_imputer.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # Report missing values before and after\n",
    "    print(\"Missing Values Before Imputation:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    return df, cat_imputer, num_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imputed, cat_imputer, num_imputer = detect_and_impute_missing(df, numerical_strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfa28b",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection & Treatment\n",
    "\n",
    "Implement IQR-based and Z-score methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703847b",
   "metadata": {},
   "source": [
    "### 6.1 IQR Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, num_cols, k=1.5, verbose=True):\n",
    "    df = df.copy()\n",
    "    for c in num_cols:\n",
    "        Q1 = df[c].quantile(0.25)\n",
    "        Q3 = df[c].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - k * IQR\n",
    "        upper = Q3 + k * IQR\n",
    "        before = len(df)\n",
    "        df = df[(df[c] >= lower) & (df[c] <= upper)]\n",
    "        after = len(df)\n",
    "        if verbose:\n",
    "            print(f\"Column {c}: removed {before-after} rows using IQR (k={k})\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers_iqr(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b063",
   "metadata": {},
   "source": [
    "### 6.2 Z-Score Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def remove_outliers_zscore(df, num_col, z_thresh=3.0, verbose=True):\n",
    "    df = df.copy()\n",
    "    z_scores = np.abs(stats.zscore(df[num_col].dropna()))\n",
    "    mask = (z_scores < z_thresh).all(axis=1)\n",
    "    before = len(df)\n",
    "    df = df.loc[df[num_col].dropna().index[mask]]\n",
    "    after = len(df)\n",
    "    if verbose:\n",
    "        print(f\"Removed {before-after} rows by z-score threshold {z_thresh}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers_zscore(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075582eb",
   "metadata": {},
   "source": [
    "### 6.3 Handling with Log Transfor like Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like Before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97787e85",
   "metadata": {},
   "source": [
    "### 6.4 Handling with Robust Scaler\n",
    "\n",
    "It is useful when the data contains outliers that cannot or should not be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def apply_robust_scaler(df, numeric_columns):\n",
    "    \"\"\"Apply RobustScaler to numerical columns.\"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae040cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_df = apply_robust_scaler(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366d4bc",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Examples: ratio features, date extraction, interaction terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15153d94",
   "metadata": {},
   "source": [
    "### 7.1 Ratio Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc823d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ratio_feature(df, numerator, denominator, new_name=None):\n",
    "    df = df.copy()\n",
    "    new_name = new_name or f\"{numerator}_over_{denominator}\"\n",
    "    df[new_name] = df[numerator] / (df[denominator].replace(0, np.nan) + 1e-9)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = add_ratio_feature(df, 'feature1', 'feature2', new_name='feature_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce26b30",
   "metadata": {},
   "source": [
    "### 7.2 Date Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211de4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_parts(df, date_col):\n",
    "    df = df.copy()\n",
    "    dt = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    df[f\"{date_col}_year\"] = dt.dt.year\n",
    "    df[f\"{date_col}_month\"] = dt.dt.month\n",
    "    df[f\"{date_col}_day\"] = dt.dt.day\n",
    "    df[f\"{date_col}_weekday\"] = dt.dt.weekday\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03dd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_date_parts(df, 'date_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63f2c4",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n",
    "\n",
    "Selecting the most important features improves model performance and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051dead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change target column\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e053160",
   "metadata": {},
   "source": [
    "### 8.1 SelectKBest (Univariate Selection)\n",
    "\n",
    "Selects top K features based on statistical tests.\n",
    "\n",
    "- chi2 → for non-negative features (e.g., counts, frequencies).\n",
    "- f_classif → for continuous numerical features in classification problems.\n",
    "- Useful for quick filtering before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "def select_features_statistical(X, y, method, k):\n",
    "    selector = SelectKBest(score_func=method, k=k)\n",
    "    selector.fit_transform(X, y)\n",
    "\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(f\"Selected Top {k} Features:\")\n",
    "    print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_statistical(X, y, method=f_classif, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf53886",
   "metadata": {},
   "source": [
    "### 8.2 Recursive Feature Elimination (RFE)\n",
    "\n",
    "Iteratively trains a model and removes the least important features.\n",
    "\n",
    "- More computationally expensive.\n",
    "- Works best when you have a moderate number of features (< 100).\n",
    "- Can be used with any estimator that exposes a coef_ or feature_importances_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Apply RFE\n",
    "rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "selected_features_rfe = X.columns[rfe.support_]\n",
    "print(\"Selected Features using RFE:\")\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd23a6",
   "metadata": {},
   "source": [
    "### 8.3 Feature Importance (Tree-based Models)\n",
    "\n",
    "Uses built-in feature importance scores from tree-based models (e.g., Random Forest, XGBoost).\n",
    "\n",
    "- Works only with tree-based models\n",
    "- Captures non-linear relationships.\n",
    "- Provides insights into feature relationships and importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7de9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "selected_features_rf = X.columns[indices[:10]]\n",
    "\n",
    "print(\"Top 10 Important Features (Random Forest):\")\n",
    "print(selected_features_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(X.columns[indices[:10]], importances[indices[:10]])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0f207",
   "metadata": {},
   "source": [
    "## 9. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba178e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Train set size:\", X_train.shape)\n",
    "print(\"X Validation set size:\", X_val.shape)\n",
    "print(\"X Test set size:\", X_test.shape)\n",
    "\n",
    "print(\"y Train set size:\", y_train.shape)\n",
    "print(\"y Validation set size:\", y_val.shape)\n",
    "print(\"y Test set size:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fd4e3",
   "metadata": {},
   "source": [
    "## 10. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e164b21",
   "metadata": {},
   "source": [
    "### 10.1 One-Hot Encoding\n",
    "Best for categorical features without ordinal relationship (e.g. color, city)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97275e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "color_encoded_train = encoder.fit_transform(X_train[['color']])\n",
    "color_encoded_val = encoder.transform(X_val[['color']])\n",
    "color_encoded_test = encoder.transform(X_test[['color']])\n",
    "\n",
    "# drop original categorical columns\n",
    "X_train = X_train.drop(columns=['color'])\n",
    "X_val = X_val.drop(columns=['color'])\n",
    "X_test = X_test.drop(columns=['color'])\n",
    "\n",
    "# concatenate the encoded features with the original dataframe\n",
    "X_train = X_train.join(pd.DataFrame(color_encoded_train, columns=encoder.get_feature_names_out(['color'])))\n",
    "X_val = X_val.join(pd.DataFrame(color_encoded_val, columns=encoder.get_feature_names_out(['color'])))\n",
    "X_test = X_test.join(pd.DataFrame(color_encoded_test, columns=encoder.get_feature_names_out(['color'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17a871",
   "metadata": {},
   "source": [
    "### 10.2 Label Encoding\n",
    "Best for binary or nominal categorical features (e.g. gender)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc728d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_train['gender_encoded'] = le.fit_transform(X_train['gender'])\n",
    "X_train.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "X_val['gender_encoded'] = le.transform(X_val['gender'])\n",
    "X_val.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "X_test['gender_encoded'] = le.transform(X_test['gender'])\n",
    "X_test.drop('gender', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e028f9",
   "metadata": {},
   "source": [
    "### 10.3 Ordinal Encoding\n",
    "Best for ordinal categorical features (e.g. education level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder(categories=[['High School','Bachelor','Master','PhD']])\n",
    "X_train['education_encoded'] = encoder.fit_transform(X_train[['education']])\n",
    "X_train.drop('education', axis=1, inplace=True)\n",
    "\n",
    "X_val['education_encoded'] = encoder.transform(X_val[['education']])\n",
    "X_val.drop('education', axis=1, inplace=True)\n",
    "\n",
    "X_test['education_encoded'] = encoder.transform(X_test[['education']])\n",
    "X_test.drop('education', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d3fea",
   "metadata": {},
   "source": [
    "### 10.4 Frequency Encoding\n",
    "\n",
    "Used for categorical columns with many unique values (high cardinality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d41677",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = X_train['category'].value_counts(normalize=True)\n",
    "X_train['category_freq_enc'] = X_train['category'].map(freq)\n",
    "X_train = X_train.drop(columns=['category'])\n",
    "\n",
    "X_val['category_freq_enc'] = X_val['category'].map(freq)\n",
    "X_val = X_val.drop(columns=['category'])\n",
    "\n",
    "X_test['category_freq_enc'] = X_test['category'].map(freq)\n",
    "X_test = X_test.drop(columns=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f4979f",
   "metadata": {},
   "source": [
    "### 10.5 MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc39cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def apply_multilabel_binarizer(df, column_name, separator='|', prefix=None):\n",
    "    \"\"\"Encode multi-label column into binary columns using MultiLabelBinarizer.\"\"\"\n",
    "    if prefix is None:\n",
    "        prefix = column_name\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values and split labels\n",
    "    df[column_name] = df[column_name].fillna('').str.split(separator).apply(lambda x: [label.strip() for label in x if label.strip()])\n",
    "    \n",
    "    # Apply MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb_matrix = mlb.fit_transform(df[column_name])\n",
    "    \n",
    "    # Create binary columns\n",
    "    mlb_cols = [f'{prefix}_{label}' for label in mlb.classes_]\n",
    "    df_mlb = pd.DataFrame(mlb_matrix, columns=mlb_cols, index=df.index)\n",
    "    df = pd.concat([df, df_mlb], axis=1)\n",
    "    \n",
    "    return df, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d33dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, mlb_model = apply_multilabel_binarizer(df, 'tags', separator='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c60da",
   "metadata": {},
   "source": [
    "## 11. Numerical Feature Scaling\n",
    "\n",
    "Choose scaler depending on data distribution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebbf20",
   "metadata": {},
   "source": [
    "### 11.1 StandardScaler\n",
    "\n",
    "Useful when features follow a **Gaussian distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = scaler.transform(X_val[numeric_columns])\n",
    "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3391e6",
   "metadata": {},
   "source": [
    "### 11.2 MinMaxScaler\n",
    "\n",
    "Useful when features have **different scales** but known **min/max ranges**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e834d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train[\"name_of_num_col\"] = scaler.fit_transform(X_train[\"name_of_num_col\"])\n",
    "X_val[\"name_of_num_col\"] = scaler.transform(X_val[\"name_of_num_col\"])\n",
    "X_test[\"name_of_num_col\"] = scaler.transform(X_test[\"name_of_num_col\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef9e10",
   "metadata": {},
   "source": [
    "### 11.3 RobustScaler\n",
    "\n",
    "Useful for data with **outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ffb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train['name_of_num_col'] = scaler.fit_transform(X_train['name_of_num_col'])\n",
    "X_val['name_of_num_col'] = scaler.transform(X_val['name_of_num_col'])\n",
    "X_test['name_of_num_col'] = scaler.transform(X_test['name_of_num_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72993ecc",
   "metadata": {},
   "source": [
    "### 11.4 Log Transformation\n",
    "\n",
    "For **skewed data** to make it more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train['name_of_num_col'] = np.log1p(X_train['name_of_num_col'])\n",
    "X_val['name_of_num_col'] = np.log1p(X_val['name_of_num_col'])\n",
    "X_test['name_of_num_col'] = np.log1p(X_test['name_of_num_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f720b",
   "metadata": {},
   "source": [
    "## 12. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10315af7",
   "metadata": {},
   "source": [
    "### Common Models — Short Description and Usage\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  Description: Linear model for binary or multiclass classification; fast, interpretable, assumes linear boundaries.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  model = LogisticRegression(random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Decision Tree**  \n",
    "  Description: Tree-based model capturing non-linear relationships; easy to visualize, prone to overfitting.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Random Forest**  \n",
    "  Description: Ensemble of decision trees (bagging); robust, reduces overfitting, good default choice.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Gradient Boosting (sklearn)**  \n",
    "  Description: Sequential tree boosting; high accuracy, needs tuning, captures complex patterns.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "  model = GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **XGBoost**  \n",
    "  Description: Optimized gradient boosting; fast, scalable, excels on tabular data, requires tuning.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from xgboost import XGBClassifier\n",
    "  model = XGBClassifier(random_state=42, n_estimators=100, eval_metric='logloss')\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Support Vector Machine (SVM)**  \n",
    "  Description: Effective in high-dimensional spaces with kernel trick for non-linear boundaries; needs scaling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.svm import SVC\n",
    "  model = SVC(kernel='rbf', C=1.0, probability=True)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**  \n",
    "  Description: Non-parametric, instance-based; simple, slow at prediction, sensitive to scaling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  model = KNeighborsClassifier(n_neighbors=5)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Naive Bayes (Gaussian)**  \n",
    "  Description: Probabilistic, assumes feature independence; fast, works well for text or small datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  model = GaussianNB()\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Multilayer Perceptron (MLP)**  \n",
    "  Description: Feedforward neural network for non-linear mappings; needs tuning and scaling, computationally intensive.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "  model = MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=300)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Recurrent Neural Network (RNN)**  \n",
    "  Description: Neural network for sequential data; captures temporal dependencies, suitable for time series or text.  \n",
    "  Usage (Keras):  \n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "  model = Sequential([\n",
    "      SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "  model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "  ```\n",
    "\n",
    "- **Long Short-Term Memory (LSTM)**  \n",
    "  Description: Advanced RNN variant; handles long-term dependencies, ideal for complex sequential data.  \n",
    "  Usage (Keras):  \n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import LSTM, Dense\n",
    "  model = Sequential([\n",
    "      LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "  model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "  ```\n",
    "\n",
    "- **Linear Regression / Ridge / Lasso** (Regression)  \n",
    "  Description: Linear models for continuous targets; Ridge/Lasso add regularization to prevent overfitting.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.linear_model import Ridge\n",
    "  model = Ridge(alpha=1.0)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Support Vector Regressor (SVR)**  \n",
    "  Description: Non-linear regression with kernels; robust but sensitive to scaling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.svm import SVR\n",
    "  model = SVR(kernel='rbf', C=1.0)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "- **K-Means (Clustering)**  \n",
    "  Description: Partitions data into k clusters by centroid assignment; fast, assumes spherical clusters.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.cluster import KMeans\n",
    "  km = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "  labels = km.labels_\n",
    "  ```\n",
    "\n",
    "- **DBSCAN (Clustering)**  \n",
    "  Description: Density-based clustering; finds arbitrary-shaped clusters, handles noise, no need to specify k.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.cluster import DBSCAN\n",
    "  db = DBSCAN(eps=0.5, min_samples=5).fit(X)\n",
    "  labels = db.labels_\n",
    "  ```\n",
    "\n",
    "- **Isolation Forest (Anomaly Detection)**  \n",
    "  Description: Tree-based anomaly detection; isolates outliers by random splits, effective for high-dimensional data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.ensemble import IsolationForest\n",
    "  iso = IsolationForest(contamination=0.1, random_state=42).fit(X)\n",
    "  anomalies = iso.predict(X)  # -1 for outliers, 1 for inliers\n",
    "  ```\n",
    "\n",
    "- **Autoencoder (Dimensionality Reduction / Anomaly Detection)**  \n",
    "  Description: Neural network for unsupervised feature learning; compresses data or detects anomalies via reconstruction error.  \n",
    "  Usage (Keras):  \n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import Dense\n",
    "  model = Sequential([\n",
    "      Dense(32, activation='relu', input_shape=(X.shape[1],)),\n",
    "      Dense(16, activation='relu'),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dense(X.shape[1], activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='mse')\n",
    "  model.fit(X, X, epochs=10, batch_size=32)\n",
    "  ```\n",
    "\n",
    "- **PCA (Dimensionality Reduction)**  \n",
    "  Description: Linear projection to principal components; used for compression or visualization.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.decomposition import PCA\n",
    "  pca = PCA(n_components=2).fit(X)\n",
    "  X_reduced = pca.transform(X)\n",
    "  ```\n",
    "    ```\n",
    "\n",
    "Notes: choose models based on problem type (classification/regression), data size, feature scaling, interpretability needs, and compute budget. Always validate with cross-validation and tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18be9e",
   "metadata": {},
   "source": [
    "## 13.Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbe8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d3e1a",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "- **Mean Squared Error (MSE)**  \n",
    "  Description: Average of squared differences between predictions and actual values; penalizes larger errors heavily.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  mse = mean_squared_error(y_true, y_pred)\n",
    "  print(f\"MSE: {mse:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**  \n",
    "  Description: Square root of MSE; interpretable in the same units as the target.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "  print(f\"RMSE: {rmse:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Mean Absolute Error (MAE)**  \n",
    "  Description: Average of absolute differences; less sensitive to outliers than MSE.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  print(f\"MAE: {mae:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **R² Score (Coefficient of Determination)**  \n",
    "  Description: Proportion of variance explained by the model; ranges from 0 to 1 (higher is better).  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  print(f\"R² Score: {r2:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Residual Plot**  \n",
    "  Description: Visualizes prediction errors (residuals) to assess model fit; ideally, residuals are randomly scattered around 0.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  residuals = y_true - y_pred\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "  plt.axhline(y=0, color='r', linestyle='--')\n",
    "  plt.xlabel('Predicted Values')\n",
    "  plt.ylabel('Residuals')\n",
    "  plt.title('Residual Plot')\n",
    "  plt.show()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76869996",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "- **Accuracy**  \n",
    "  Description: Proportion of correct predictions; good for balanced datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  accuracy = accuracy_score(y_true, y_pred)\n",
    "  print(f\"Accuracy: {accuracy:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Precision**  \n",
    "  Description: Proportion of true positives among positive predictions; useful when false positives are costly.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  precision = precision_score(y_true, y_pred, average='weighted')  # Use 'binary' for binary classification\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Recall (Sensitivity)**  \n",
    "  Description: Proportion of true positives identified; critical when false negatives are costly.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  recall = recall_score(y_true, y_pred, average='weighted')  # Use 'binary' for binary classification\n",
    "  print(f\"Recall: {recall:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **F1 Score**  \n",
    "  Description: Harmonic mean of precision and recall; balances both for imbalanced datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  f1 = f1_score(y_true, y_pred, average='weighted')  # Use 'binary' for binary classification\n",
    "  print(f\"F1 Score: {f1:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **ROC-AUC Score**  \n",
    "  Description: Area under the ROC curve; measures model's ability to distinguish classes (binary/multiclass). Requires probability scores.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')  # Use y_pred_proba for probabilities\n",
    "  print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Confusion Matrix**  \n",
    "  Description: Table showing true vs. predicted class counts; helps visualize classification performance.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  plt.figure(figsize=(6, 4))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('True')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172027b",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, TomekLinks, EditedNearestNeighbours\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda4553",
   "metadata": {},
   "source": [
    "## Oversampling Techniques\n",
    "\n",
    "- **Random Oversampling**  \n",
    "  Description: Randomly duplicates samples from the minority class to balance the dataset; simple but risks overfitting.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  ros = RandomOverSampler(random_state=42)\n",
    "  X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "  print(\"Class distribution after Random Oversampling:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **SMOTE (Synthetic Minority Oversampling Technique)**  \n",
    "  Description: Generates synthetic samples for the minority class by interpolating between existing samples; reduces overfitting compared to random oversampling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  smote = SMOTE(random_state=42)\n",
    "  X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "  print(\"Class distribution after SMOTE:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **ADASYN (Adaptive Synthetic Sampling)**  \n",
    "  Description: Similar to SMOTE but focuses synthetic samples on harder-to-classify regions (near class boundaries); good for complex datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  adasyn = ADASYN(random_state=42)\n",
    "  X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "  print(\"Class distribution after ADASYN:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "## Undersampling Techniques\n",
    "\n",
    "- **Random Undersampling**  \n",
    "  Description: Randomly removes samples from the majority class to balance the dataset; simple but may discard valuable data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  rus = RandomUnderSampler(random_state=42)\n",
    "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "  print(\"Class distribution after Random Undersampling:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **Cluster Centroids**  \n",
    "  Description: Replaces majority class samples with centroids of clusters; preserves structure but may oversimplify data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  cc = ClusterCentroids(random_state=42)\n",
    "  X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "  print(\"Class distribution after Cluster Centroids:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **Tomek Links**  \n",
    "  Description: Removes majority class samples that are closest to minority class samples (links); improves class separation.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  tl = TomekLinks()\n",
    "  X_resampled, y_resampled = tl.fit_resample(X, y)\n",
    "  print(\"Class distribution after Tomek Links:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **Edited Nearest Neighbors (ENN)**  \n",
    "  Description: Removes majority class samples misclassified by their k-nearest neighbors; cleans noisy data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  enn = EditedNearestNeighbours()\n",
    "  X_resampled, y_resampled = enn.fit_resample(X, y)\n",
    "  print(\"Class distribution after ENN:\", pd.Series(y_resampled).value_counts())\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
