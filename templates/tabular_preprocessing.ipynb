{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33343b9a",
   "metadata": {},
   "source": [
    "# TABULAR DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07409bb6",
   "metadata": {},
   "source": [
    "## 1. Imports and settings\n",
    "\n",
    "Import required libraries and configure display options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aad895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91b4a5",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n",
    "\n",
    "Load a CSV file into a DataFrame. Change DATA_PATH to your file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1417504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tabular_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3cfcf",
   "metadata": {},
   "source": [
    "## 3. Initial inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ac3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21093bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c36afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e272ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21427455",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize numerical and categorical distributions, boxplots for outliers, and correlation heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37174b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d424498",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "categorical_columns = df.select_dtypes(include=['object','category','bool']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21780edd",
   "metadata": {},
   "source": [
    "### 4.1 Numerical features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c67183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distributions(df, num_cols, bins=50):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=bins)\n",
    "        plt.title(f'Distribution: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_numeric_distributions(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc8a27",
   "metadata": {},
   "source": [
    "### 4.2 Categorical features count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_counts(df, cat_cols, top_n=20):\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.countplot(y=df[col], order=df[col].value_counts().index[:top_n])\n",
    "        plt.title(f'Counts: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024df5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_categorical_counts(df, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4eef1",
   "metadata": {},
   "source": [
    "### 4.3 Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df, num_cols):\n",
    "    corr = df[num_cols].corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_heatmap(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb0b54",
   "metadata": {},
   "source": [
    "### 4.4 Boxplot to check outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c97659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, num_cols):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(6,2))\n",
    "        sns.boxplot(x=df[col].dropna())\n",
    "        plt.title(f'Boxplot: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e744e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_boxplots(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f25152",
   "metadata": {},
   "source": [
    "## 5. Missing Value Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7506e",
   "metadata": {},
   "source": [
    "### 5.1 Drop Columns\n",
    "Use drop_columns when you want to drop entire useless columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    \"\"\"Drop specified columns from the DataFrame.\"\"\"\n",
    "    return df.drop(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ed56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_columns(df, ['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a052b1d",
   "metadata": {},
   "source": [
    "### 5.2 Drop Rows\n",
    "Use drop_rows when the percentage of missing data is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3efb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(df):\n",
    "    \"\"\"\n",
    "    Remove rows with missing values.\n",
    "    \"\"\"\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = drop_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5251d5",
   "metadata": {},
   "source": [
    "### 5.3 Statistical Imputation\n",
    "Use fill_with_statistical for numerical features when distribution is stable.\n",
    "> Note: Before use **statistical imputation** split your Dataset, because of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_statistical(df, num_col, strategy=\"mean\"):\n",
    "    \"\"\"Fill missing with mean, median or mode.\"\"\"\n",
    "    if strategy == \"mean\":\n",
    "        df[num_col] = df[num_col].fillna(df[num_col].mean())\n",
    "    elif strategy == \"median\":\n",
    "        df[num_col] = df[num_col].fillna(df[num_col].median())\n",
    "    elif strategy == \"mode\":\n",
    "        df[num_col] = df[num_col].fillna(df[num_col].mode().iloc[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_with_statistical(df, columns=['num_col1', 'num_col2'], method=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6c7f1",
   "metadata": {},
   "source": [
    "### 5.4 Categorical Imputation\n",
    "Use fill_categorical for categorical features.\n",
    "> Note: Before use **categorical imputation** split your Dataset, because of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical(df, cat_cols):\n",
    "    \"\"\"Fill missing categorical values with mode.\"\"\"\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1dc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_categorical(df, cat_cols=['cat_col1', 'cat_col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407b432",
   "metadata": {},
   "source": [
    "### 5.5 Forward and Backward Fill\n",
    "Use interpolation for time-series or continuous numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_ffill_bfill(df, columns, method=\"ffill\"):\n",
    "    \"\"\"Fill using forward fill or backward fill.\"\"\"\n",
    "    df[columns] = df[columns].fillna(method=method)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7681cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_with_ffill_bfill(df, columns=['num_col1', 'num_col2'], method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf00cbf",
   "metadata": {},
   "source": [
    "### 5.6 Imputation Techniques\n",
    "Use KNN imputation when you expect relationships between features.\n",
    "> Note: Before use **KNN imputation** split your Dataset, because of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2890ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def fill_with_knn(df, numeric_columns, n_neighbors=3):\n",
    "    \"\"\"Impute missing values using KNN.\"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fill_with_knn(df, numeric_columns=['num_col1', 'num_col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfa28b",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection & Treatment\n",
    "\n",
    "Implement IQR-based and Z-score methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703847b",
   "metadata": {},
   "source": [
    "### 6.1 IQR Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, num_cols, k=1.5, verbose=True):\n",
    "    df = df.copy()\n",
    "    for c in num_cols:\n",
    "        Q1 = df[c].quantile(0.25)\n",
    "        Q3 = df[c].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - k * IQR\n",
    "        upper = Q3 + k * IQR\n",
    "        before = len(df)\n",
    "        df = df[(df[c] >= lower) & (df[c] <= upper)]\n",
    "        after = len(df)\n",
    "        if verbose:\n",
    "            print(f\"Column {c}: removed {before-after} rows using IQR (k={k})\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers_iqr(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b063",
   "metadata": {},
   "source": [
    "### 6.2 Z-Score Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def remove_outliers_zscore(df, num_col, z_thresh=3.0, verbose=True):\n",
    "    df = df.copy()\n",
    "    z_scores = np.abs(stats.zscore(df[num_col].dropna()))\n",
    "    mask = (z_scores < z_thresh).all(axis=1)\n",
    "    before = len(df)\n",
    "    df = df.loc[df[num_col].dropna().index[mask]]\n",
    "    after = len(df)\n",
    "    if verbose:\n",
    "        print(f\"Removed {before-after} rows by z-score threshold {z_thresh}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers_zscore(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366d4bc",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Examples: ratio features, date extraction, interaction terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15153d94",
   "metadata": {},
   "source": [
    "### 7.1 Ratio Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc823d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ratio_feature(df, numerator, denominator, new_name=None):\n",
    "    df = df.copy()\n",
    "    new_name = new_name or f\"{numerator}_over_{denominator}\"\n",
    "    df[new_name] = df[numerator] / (df[denominator].replace(0, np.nan) + 1e-9)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = add_ratio_feature(df, 'feature1', 'feature2', new_name='feature_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce26b30",
   "metadata": {},
   "source": [
    "### 7.2 Date Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211de4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_parts(df, date_col):\n",
    "    df = df.copy()\n",
    "    dt = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    df[f\"{date_col}_year\"] = dt.dt.year\n",
    "    df[f\"{date_col}_month\"] = dt.dt.month\n",
    "    df[f\"{date_col}_day\"] = dt.dt.day\n",
    "    df[f\"{date_col}_weekday\"] = dt.dt.weekday\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03dd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_date_parts(df, 'date_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63f2c4",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n",
    "\n",
    "Selecting the most important features improves model performance and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051dead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change target column\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e053160",
   "metadata": {},
   "source": [
    "### 8.1 SelectKBest (Univariate Selection)\n",
    "\n",
    "Selects top K features based on statistical tests.\n",
    "\n",
    "- chi2 → for non-negative features (e.g., counts, frequencies).\n",
    "- f_classif → for continuous numerical features in classification problems.\n",
    "- Useful for quick filtering before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "def select_features_statistical(X, y, method, k):\n",
    "    selector = SelectKBest(score_func=method, k=k)\n",
    "    selector.fit_transform(X, y)\n",
    "\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(f\"Selected Top {k} Features:\")\n",
    "    print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_statistical(X, y, method=f_classif, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf53886",
   "metadata": {},
   "source": [
    "### 8.2 Recursive Feature Elimination (RFE)\n",
    "\n",
    "Iteratively trains a model and removes the least important features.\n",
    "\n",
    "- More computationally expensive.\n",
    "- Works best when you have a moderate number of features (< 100).\n",
    "- Can be used with any estimator that exposes a coef_ or feature_importances_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Apply RFE\n",
    "rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "selected_features_rfe = X.columns[rfe.support_]\n",
    "print(\"Selected Features using RFE:\")\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd23a6",
   "metadata": {},
   "source": [
    "### 8.3 Feature Importance (Tree-based Models)\n",
    "\n",
    "Uses built-in feature importance scores from tree-based models (e.g., Random Forest, XGBoost).\n",
    "\n",
    "- Works only with tree-based models\n",
    "- Captures non-linear relationships.\n",
    "- Provides insights into feature relationships and importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7de9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "selected_features_rf = X.columns[indices[:10]]\n",
    "\n",
    "print(\"Top 10 Important Features (Random Forest):\")\n",
    "print(selected_features_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(X.columns[indices[:10]], importances[indices[:10]])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0f207",
   "metadata": {},
   "source": [
    "## 9. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba178e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Train set size:\", X_train.shape)\n",
    "print(\"X Validation set size:\", X_val.shape)\n",
    "print(\"X Test set size:\", X_test.shape)\n",
    "\n",
    "print(\"y Train set size:\", y_train.shape)\n",
    "print(\"y Validation set size:\", y_val.shape)\n",
    "print(\"y Test set size:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fd4e3",
   "metadata": {},
   "source": [
    "## 10. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e164b21",
   "metadata": {},
   "source": [
    "### 10.1 One-Hot Encoding\n",
    "Best for categorical features without ordinal relationship (e.g. color, city)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97275e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "color_encoded_train = encoder.fit_transform(X_train[['color']])\n",
    "color_encoded_val = encoder.transform(X_val[['color']])\n",
    "color_encoded_test = encoder.transform(X_test[['color']])\n",
    "\n",
    "# drop original categorical columns\n",
    "X_train = X_train.drop(columns=['color'])\n",
    "X_val = X_val.drop(columns=['color'])\n",
    "X_test = X_test.drop(columns=['color'])\n",
    "\n",
    "# concatenate the encoded features with the original dataframe\n",
    "X_train = X_train.join(pd.DataFrame(color_encoded_train, columns=encoder.get_feature_names_out(['color'])))\n",
    "X_val = X_val.join(pd.DataFrame(color_encoded_val, columns=encoder.get_feature_names_out(['color'])))\n",
    "X_test = X_test.join(pd.DataFrame(color_encoded_test, columns=encoder.get_feature_names_out(['color'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17a871",
   "metadata": {},
   "source": [
    "### 10.2 Label Encoding\n",
    "Best for binary or nominal categorical features (e.g. gender)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc728d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_train['gender_encoded'] = le.fit_transform(X_train['gender'])\n",
    "X_train.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "X_val['gender_encoded'] = le.transform(X_val['gender'])\n",
    "X_val.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "X_test['gender_encoded'] = le.transform(X_test['gender'])\n",
    "X_test.drop('gender', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e028f9",
   "metadata": {},
   "source": [
    "### 10.3 Ordinal Encoding\n",
    "Best for ordinal categorical features (e.g. education level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder(categories=[['High School','Bachelor','Master','PhD']])\n",
    "X_train['education_encoded'] = encoder.fit_transform(X_train[['education']])\n",
    "X_train.drop('education', axis=1, inplace=True)\n",
    "\n",
    "X_val['education_encoded'] = encoder.transform(X_val[['education']])\n",
    "X_val.drop('education', axis=1, inplace=True)\n",
    "\n",
    "X_test['education_encoded'] = encoder.transform(X_test[['education']])\n",
    "X_test.drop('education', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d3fea",
   "metadata": {},
   "source": [
    "### 10.4 Frequency Encoding\n",
    "\n",
    "Used for categorical columns with many unique values (high cardinality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d41677",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = X_train['category'].value_counts(normalize=True)\n",
    "X_train['category_freq_enc'] = X_train['category'].map(freq)\n",
    "X_train = X_train.drop(columns=['category'])\n",
    "\n",
    "X_val['category_freq_enc'] = X_val['category'].map(freq)\n",
    "X_val = X_val.drop(columns=['category'])\n",
    "\n",
    "X_test['category_freq_enc'] = X_test['category'].map(freq)\n",
    "X_test = X_test.drop(columns=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c60da",
   "metadata": {},
   "source": [
    "## 11. Numerical Feature Scaling\n",
    "\n",
    "Choose scaler depending on data distribution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebbf20",
   "metadata": {},
   "source": [
    "### 11.1 StandardScaler\n",
    "\n",
    "Useful when features follow a **Gaussian distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = scaler.transform(X_val[numeric_columns])\n",
    "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3391e6",
   "metadata": {},
   "source": [
    "### 11.2 MinMaxScaler\n",
    "\n",
    "Useful when features have **different scales** but known **min/max ranges**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e834d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train[\"name_of_num_col\"] = scaler.fit_transform(X_train[\"name_of_num_col\"])\n",
    "X_val[\"name_of_num_col\"] = scaler.transform(X_val[\"name_of_num_col\"])\n",
    "X_test[\"name_of_num_col\"] = scaler.transform(X_test[\"name_of_num_col\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef9e10",
   "metadata": {},
   "source": [
    "### 11.3 RobustScaler\n",
    "\n",
    "Useful for data with **outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ffb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train['name_of_num_col'] = scaler.fit_transform(X_train['name_of_num_col'])\n",
    "X_val['name_of_num_col'] = scaler.transform(X_val['name_of_num_col'])\n",
    "X_test['name_of_num_col'] = scaler.transform(X_test['name_of_num_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72993ecc",
   "metadata": {},
   "source": [
    "### 11.4 Log Transformation\n",
    "\n",
    "For **skewed data** to make it more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train['name_of_num_col'] = np.log1p(X_train['name_of_num_col'])\n",
    "X_val['name_of_num_col'] = np.log1p(X_val['name_of_num_col'])\n",
    "X_test['name_of_num_col'] = np.log1p(X_test['name_of_num_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f720b",
   "metadata": {},
   "source": [
    "### 12. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10315af7",
   "metadata": {},
   "source": [
    "### Common models — short description and usage\n",
    "\n",
    "- **Logistic Regression**  \n",
    "    Description: Linear model for binary (or multiclass via one-vs-rest) classification; fast, interpretable.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Decision Tree**  \n",
    "    Description: Tree-based model that captures non-linear relationships and interactions; easy to visualize, prone to overfitting.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Random Forest**  \n",
    "    Description: Ensemble of decision trees (bagging); robust, less overfitting than single trees, good out-of-the-box.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Gradient Boosting (sklearn / XGBoost / LightGBM / CatBoost)**  \n",
    "    Description: Sequential tree boosting; often higher accuracy than random forest, requires tuning, handles complex patterns.  \n",
    "    Usage (sklearn):\n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    model = GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "    For speed/large data prefer XGBoost/LightGBM/CatBoost APIs.\n",
    "\n",
    "- **Support Vector Machine (SVM)**  \n",
    "    Description: Effective in high-dimensional spaces, with kernel trick for non-linear decision boundaries; sensitive to feature scaling.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='rbf', C=1.0, probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**  \n",
    "    Description: Instance-based, non-parametric; simple, no training time but expensive at predict time, sensitive to scaling and noise.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Naive Bayes (Gaussian/Bernoulli/Multinomial)**  \n",
    "    Description: Probabilistic classifiers assuming feature independence; very fast and works well on text/low-sample problems.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Multilayer Perceptron (Neural Network)**  \n",
    "    Description: Feedforward neural network for complex non-linear mappings; needs tuning and normalization, can be slow on large data.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=300)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Linear Regression / Ridge / Lasso** (regression)  \n",
    "    Description: Baseline linear models for continuous targets; Ridge/Lasso add regularization to control overfitting.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **Support Vector Regressor (SVR) / KNN Regressor** (regression)  \n",
    "    Description: SVR for robust non-linear regression (with kernels); KNNRegressor for simple non-parametric regression.  \n",
    "    Usage (SVR):\n",
    "    ```python\n",
    "    from sklearn.svm import SVR\n",
    "    model = SVR(kernel='rbf', C=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- **K-Means (clustering)**  \n",
    "    Description: Unsupervised clustering by centroid assignment; fast but assumes spherical clusters and requires k.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "    labels = km.labels_\n",
    "    ```\n",
    "\n",
    "- **PCA (dimensionality reduction)**  \n",
    "    Description: Linear projection to principal components for compression, noise reduction, or visualization.  \n",
    "    Usage:\n",
    "    ```python\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2).fit(X)\n",
    "    X_reduced = pca.transform(X)\n",
    "    ```\n",
    "\n",
    "Notes: choose models based on problem type (classification/regression), data size, feature scaling, interpretability needs, and compute budget. Always validate with cross-validation and tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
