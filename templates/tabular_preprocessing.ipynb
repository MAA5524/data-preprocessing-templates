{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33343b9a",
   "metadata": {},
   "source": [
    "# TABULAR DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07409bb6",
   "metadata": {},
   "source": [
    "## 1. Imports and settings\n",
    "\n",
    "Import required libraries and configure display options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aad895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91b4a5",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1417504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3cfcf",
   "metadata": {},
   "source": [
    "## 3. Initial inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ac3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21093bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c36afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e272ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c31349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train['target'].unique()))\n",
    "print(df_train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca33ea",
   "metadata": {},
   "source": [
    "## 4. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(\"target\", axis=1)\n",
    "y = df_train[\"target\"]\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Train set size:\", X_train.shape)\n",
    "print(\"X Validation set size:\", X_val.shape)\n",
    "print(\"X Test set size:\", X_test.shape)\n",
    "\n",
    "print(\"y Train set size:\", y_train.shape)\n",
    "print(\"y Validation set size:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bed493",
   "metadata": {},
   "source": [
    "## 5. Missing Value Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfa24e",
   "metadata": {},
   "source": [
    "### 5.1 Detecting Hidden Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hidden_missing_data(df, suspicious_values=None, target_column=None):\n",
    "    \"\"\"\n",
    "    Detects missing data in the DataFrame for all columns or a specific column,\n",
    "    including standard NaN/None and custom suspicious values.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - suspicious_values: List of custom values to treat as missing (e.g., [-999, 'unknown', '']).\n",
    "                         Defaults to None (only checks NaN/None).\n",
    "    - target_column: String, name of a specific column to check for missing data.\n",
    "                    If None, checks all columns. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame summarizing missing counts and percentages for the selected column(s).\n",
    "    \"\"\"\n",
    "    if suspicious_values is None:\n",
    "        suspicious_values = []\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    temp_df = df.copy()\n",
    "\n",
    "    # Validate target_column\n",
    "    if target_column is not None:\n",
    "        if target_column not in temp_df.columns:\n",
    "            print(f\"Error: Column '{target_column}' not found in DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        temp_df = temp_df[[target_column]]\n",
    "        print(f\"Checking missing data for column: {target_column}\")\n",
    "    else:\n",
    "        print(\"Checking missing data for all columns\")\n",
    "\n",
    "    # Replace suspicious values with NaN\n",
    "    for val in suspicious_values:\n",
    "        temp_df = temp_df.replace(val, np.nan)\n",
    "\n",
    "    # Calculate missing counts and percentages\n",
    "    missing_counts = temp_df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(temp_df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing Percentage (%)': missing_percentages.round(2)\n",
    "    }).sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "    # Filter summary to show only columns with missing data\n",
    "    missing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n",
    "\n",
    "    # Print summary\n",
    "    if not missing_summary.empty:\n",
    "        print(\"Missing Data Summary:\")\n",
    "        print(missing_summary)\n",
    "    else:\n",
    "        print(\"No missing data detected in the selected column(s).\")\n",
    "\n",
    "    return missing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75900056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suspicious = [-999, 'unknown', '', 'N/A']  # Customize based on your data\n",
    "\n",
    "# # Check for a specific column\n",
    "# missing_summary_train = detect_hidden_missing_data(df_train, suspicious_values=suspicious, target_column='column_name')\n",
    "# missing_summary_val = detect_hidden_missing_data(X_val, suspicious_values=suspicious, target_column='column_name')\n",
    "# missing_summary_test = detect_hidden_missing_data(df_test, suspicious_values=suspicious, target_column='column_name')\n",
    "\n",
    "# # Check for all columns\n",
    "# missing_summary_all_train = detect_hidden_missing_data(df_train, suspicious_values=suspicious)\n",
    "# missing_summary_all_val = detect_hidden_missing_data(X_val, suspicious_values=suspicious)\n",
    "# missing_summary_all_test = detect_hidden_missing_data(df_test, suspicious_values=suspicious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c26c2",
   "metadata": {},
   "source": [
    "### 5.2 Show Unique Values and Percentages of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_and_unique_info(df):\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_percent = round((missing_count / len(df)) * 100, 2)\n",
    "\n",
    "    cols_with_missing = missing_count[missing_count > 0].index\n",
    "\n",
    "    if len(cols_with_missing) == 0:\n",
    "        print(\"any columns with missing values\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    non_null_count = df[cols_with_missing].notnull().sum()\n",
    "\n",
    "    unique_counts = pd.Series(index=cols_with_missing, dtype='object')\n",
    "    for col in cols_with_missing:\n",
    "        if df[col].dtype == 'object' or pd.api.types.is_categorical_dtype(df[col]):\n",
    "            unique_counts[col] = df[col].nunique(dropna=True)\n",
    "        else:\n",
    "            unique_counts[col] = np.nan\n",
    "\n",
    "    report = pd.DataFrame({\n",
    "        'Missing %': missing_percent[cols_with_missing],\n",
    "        'Non-null Count': non_null_count,\n",
    "        'Unique (for object/categorical)': unique_counts\n",
    "    })\n",
    "\n",
    "    report.index.name = 'Feature'\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873cde6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = missing_and_unique_info(X_train)\n",
    "result_val = missing_and_unique_info(X_val)\n",
    "result_test = missing_and_unique_info(X_test)\n",
    "\n",
    "print(result_train)\n",
    "print(result_val)\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165d449",
   "metadata": {},
   "source": [
    "### 5.3 Drop Columns\n",
    "Use drop_columns when you want to drop entire useless columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    \"\"\"Drop specified columns from the DataFrame.\"\"\"\n",
    "    df.drop(columns=columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_columns(df_train, ['col1', 'col2'])\n",
    "# drop_columns(df_val, ['col1', 'col2'])\n",
    "# drop_columns(df_test, ['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fef9b",
   "metadata": {},
   "source": [
    "### 5.4 Drop All Missing Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(df, target_column=None):\n",
    "    \"\"\"\n",
    "    Drops rows with missing values from the DataFrame, optionally filtering first by a target column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data.\n",
    "    - target_column: str or None. If provided, rows where df[target_column] is null are first removed,\n",
    "        then a global dropna() is applied to the DataFrame. If None, dropna() is applied to the entire DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: a new DataFrame with rows containing missing values removed.\n",
    "    \"\"\"\n",
    "    if target_column is not None:\n",
    "        df = df[df[target_column].notnull()]\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with missing values from the special columns of training DataFrame\n",
    "# X_train = drop_missing(X_train, target_column=['col1', 'col2'])\n",
    "# X_val = drop_missing(X_val, target_column=['col1', 'col2'])\n",
    "\n",
    "# Drop rows with missing values from the entire training DataFrame\n",
    "# X_train = drop_missing(X_train)\n",
    "# X_val = drop_missing(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55074a",
   "metadata": {},
   "source": [
    "### 5.5 Numerical Imputation\n",
    "Use fill_numerical_cols for numerical features when distribution is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b010f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numerical_cols(df, num_cols, strategy=\"mean\"):\n",
    "    \"\"\"Fill missing with mean, median or mode.\"\"\"\n",
    "    for col in num_cols:\n",
    "        if strategy == \"mean\":\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        elif strategy == \"median\":\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        elif strategy == \"mode\":\n",
    "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# X_train = fill_numerical_cols(X_train, num_col=num_cols, strategy=\"mean\")\n",
    "# X_val = fill_numerical_cols(X_val, num_col=num_cols, strategy=\"mean\")\n",
    "# X_test = fill_numerical_cols(X_test, num_col=num_cols, strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548a16c",
   "metadata": {},
   "source": [
    "### 5.6 Categorical Imputation\n",
    "Use fill_categorical for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e30eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical(df, cat_cols):\n",
    "    \"\"\"Fill missing categorical values with mode.\"\"\"\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols = X_train.select_dtypes(include=['object']).columns\n",
    "# X_train = fill_categorical(X_train, cat_cols=cat_cols)\n",
    "# X_val = fill_categorical(X_val, cat_cols=cat_cols)\n",
    "# X_test = fill_categorical(X_test, cat_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2c1ea",
   "metadata": {},
   "source": [
    "### 5.7 Forward and Backward Fill\n",
    "Use interpolation for time-series or continuous numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2dd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_ffill_bfill(df, columns, method=\"ffill\"):\n",
    "    \"\"\"Fill using forward fill or backward fill.\"\"\"\n",
    "    df[columns] = df[columns].fillna(method=method)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b84307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = fill_with_ffill_bfill(X_train, columns=['num_col1', 'num_col2'], method=\"ffill\")\n",
    "# X_val = fill_with_ffill_bfill(X_val, columns=['num_col1', 'num_col2'], method=\"ffill\")\n",
    "# X_test = fill_with_ffill_bfill(X_test, columns=['num_col1', 'num_col2'], method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df86b0f",
   "metadata": {},
   "source": [
    "### 5.8 Imputation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd79051",
   "metadata": {},
   "source": [
    "#### 5.8.1 KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "numeric_columns = ['num_col1', 'num_col2']\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "X_train[numeric_columns] = imputer.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = imputer.transform(X_val[numeric_columns])\n",
    "X_test[numeric_columns] = imputer.transform(X_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d60e1",
   "metadata": {},
   "source": [
    "#### 5.8.2 Iterative Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca403e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LinearRegression(),\n",
    "    max_iter=10,\n",
    "    random_state=42,\n",
    "    skip_complete=True\n",
    ")\n",
    "\n",
    "X_train[numeric_columns] = imputer.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = imputer.transform(X_val[numeric_columns])\n",
    "X_test[numeric_columns] = imputer.transform(X_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# X_train = fill_with_iterative_imputer(X_train, numeric_columns, is_train=True, verbose=True)\n",
    "# X_val = fill_with_iterative_imputer(X_val, numeric_columns, is_train=False, verbose=True)\n",
    "# X_test = fill_with_iterative_imputer(X_test, numeric_columns, is_train=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21427455",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize numerical and categorical distributions, boxplots for outliers, and correlation heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d424498",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = X_train.select_dtypes(include=np.number).columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object','category','bool']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21780edd",
   "metadata": {},
   "source": [
    "### 6.1 Numerical features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c67183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distributions(df, num_cols, bins=50):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=bins)\n",
    "        plt.title(f'Distribution: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_numeric_distributions(X_train, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc8a27",
   "metadata": {},
   "source": [
    "### 6.2 Categorical features count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_counts(df, cat_cols, top_n=20):\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.countplot(y=df[col], order=df[col].value_counts().index[:top_n])\n",
    "        plt.title(f'Counts: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024df5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_categorical_counts(X_train, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4eef1",
   "metadata": {},
   "source": [
    "### 6.3 Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df, num_cols):\n",
    "    corr = df[num_cols].corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_heatmap(X_train, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d465be",
   "metadata": {},
   "source": [
    "### 6.4 Pre Processing Date Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08768500",
   "metadata": {},
   "source": [
    "#### 6.4.1 Date Column Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53fb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datetime_columns(df, date_cols, format=None):\n",
    "    \"\"\"Convert specified columns to datetime type.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], format=format, errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_cols = ['timestamp', 'date', 'created_at']\n",
    "# X_train = parse_datetime(X_train, date_cols, format='%Y-%m-%d %H:%M:%S')\n",
    "# X_val = parse_datetime(X_val, date_cols, format='%Y-%m-%d %H:%M:%S')\n",
    "# X_test = parse_datetime(X_test, date_cols, format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd72e3",
   "metadata": {},
   "source": [
    "#### 6.4.2 Impute Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = ['timestamp', 'datetime', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6effce",
   "metadata": {},
   "source": [
    "##### 6.4.2.1 ffill & bfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6364bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in time_cols:\n",
    "    X_train[col] = X_train[col].fillna(method='ffill')\n",
    "    X_val[col] = X_val[col].fillna(method='ffill')\n",
    "    X_test[col] = X_test[col].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e4dfe",
   "metadata": {},
   "source": [
    "##### 6.4.2.2 KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd95e27",
   "metadata": {},
   "source": [
    "### 6.5 Handling Skewed Numerical Features\n",
    "\n",
    "To handle skewed numerical features, check skewness (|skew| > 1 is significant) and apply a transformation if needed:\n",
    "\n",
    "- **Log**: `np.log1p()` for positive, right-skewed data (handles zeros).  \n",
    "- **Square root**: `np.sqrt()` for moderate right-skew.  \n",
    "- **Box-Cox**: from `scipy.stats.boxcox` (requires positive values).  \n",
    "- **Yeo-Johnson**: from `sklearn.preprocessing.PowerTransformer` (works with zeros/negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc6491",
   "metadata": {},
   "source": [
    "#### 6.5.1 Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b28d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "def apply_log_transform_to_all(X_train, X_val, X_test, skew_threshold=1.0, plot=True):\n",
    "    \"\"\"\n",
    "    Applies log1p transformation to skewed numerical columns (decided by X_train)\n",
    "    and returns transformed X_train, X_val, X_test.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, X_val, X_test: DataFrames\n",
    "    - skew_threshold: threshold for |skew| to consider a column skewed (default: 1.0)\n",
    "    - plot: if True, shows before/after histograms for transformed columns (default: True)\n",
    "\n",
    "    Returns:\n",
    "    - Transformed (X_train, X_val, X_test)\n",
    "    \"\"\"\n",
    "    X_train_orig = X_train.copy()\n",
    "\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    skewed_cols = []\n",
    "    for col in numeric_cols:\n",
    "        col_skew = skew(X_train[col].dropna())\n",
    "        if abs(col_skew) > skew_threshold and (X_train[col] >= 0).all():\n",
    "            skewed_cols.append(col)\n",
    "            if plot:\n",
    "                print(f\"Applying log1p to '{col}' (skew = {col_skew:.2f})\")\n",
    "\n",
    "    if not skewed_cols:\n",
    "        if plot:\n",
    "            print(\"No columns meet the skewness and positivity criteria for log transform.\")\n",
    "        return X_train, X_val, X_test\n",
    "\n",
    "    def _log_transform(df, cols):\n",
    "        df = df.copy()\n",
    "        for col in cols:\n",
    "            df[col] = np.log1p(df[col])\n",
    "        return df\n",
    "\n",
    "    X_train_new = _log_transform(X_train, skewed_cols)\n",
    "    X_val_new = _log_transform(X_val, skewed_cols)\n",
    "    X_test_new = _log_transform(X_test, skewed_cols)\n",
    "\n",
    "    if plot:\n",
    "        for col in skewed_cols:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            sns.histplot(X_train_orig[col].dropna(), kde=True, ax=axes[0], color='skyblue')\n",
    "            axes[0].set_title(f'Original: {col}\\nSkew = {skew(X_train_orig[col].dropna()):.2f}')\n",
    "\n",
    "            sns.histplot(X_train_new[col].dropna(), kde=True, ax=axes[1], color='lightgreen')\n",
    "            axes[1].set_title(f'Log-transformed: {col}\\nSkew = {skew(X_train_new[col].dropna()):.2f}')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return X_train_new, X_val_new, X_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, X_test = apply_log_transform_to_all(\n",
    "#     X_train, X_val, X_test,\n",
    "#     skew_threshold=1.0,\n",
    "#     plot=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905cc82",
   "metadata": {},
   "source": [
    "#### 6.5.2 Squar Root Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8793bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sqrt_transform_to_all(X_train, X_val, X_test, skew_threshold=1.0, plot=True):\n",
    "    \"\"\"\n",
    "    Applies sqrt transformation to skewed numerical columns (decided by X_train)\n",
    "    and returns transformed X_train, X_val, X_test.\n",
    "    \"\"\"\n",
    "    X_train_orig = X_train.copy()\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    skewed_cols = []\n",
    "    for col in numeric_cols:\n",
    "        col_skew = skew(X_train[col].dropna())\n",
    "        if abs(col_skew) > skew_threshold and (X_train[col] >= 0).all():\n",
    "            skewed_cols.append(col)\n",
    "            if plot:\n",
    "                print(f\"Applying sqrt to '{col}' (skew = {col_skew:.2f})\")\n",
    "\n",
    "    if not skewed_cols:\n",
    "        if plot:\n",
    "            print(\"No columns meet criteria for sqrt transform.\")\n",
    "        return X_train, X_val, X_test\n",
    "\n",
    "    def _sqrt_transform(df, cols):\n",
    "        df = df.copy()\n",
    "        for col in cols:\n",
    "            df[col] = np.sqrt(df[col])\n",
    "        return df\n",
    "\n",
    "    X_train_new = _sqrt_transform(X_train, skewed_cols)\n",
    "    X_val_new = _sqrt_transform(X_val, skewed_cols)\n",
    "    X_test_new = _sqrt_transform(X_test, skewed_cols)\n",
    "\n",
    "    if plot:\n",
    "        for col in skewed_cols:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            sns.histplot(X_train_orig[col].dropna(), kde=True, ax=axes[0], color='skyblue')\n",
    "            axes[0].set_title(f'Original: {col}\\nSkew = {skew(X_train_orig[col].dropna()):.2f}')\n",
    "\n",
    "            sns.histplot(X_train_new[col].dropna(), kde=True, ax=axes[1], color='lightgreen')\n",
    "            axes[1].set_title(f'Sqrt-transformed: {col}\\nSkew = {skew(X_train_new[col].dropna()):.2f}')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return X_train_new, X_val_new, X_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770627ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, X_test = apply_sqrt_transform_to_all(\n",
    "#     X_train, X_val, X_test,\n",
    "#     skew_threshold=1.0,\n",
    "#     plot=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657eee92",
   "metadata": {},
   "source": [
    "#### 6.5.3 Boxcox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "def apply_boxcox_to_all(X_train, X_val, X_test, skew_threshold=1.0, plot=True):\n",
    "    \"\"\"\n",
    "    Applies Box-Cox transformation to skewed numerical columns.\n",
    "    Decision based on X_train; same λ used for all.\n",
    "    \"\"\"\n",
    "    X_train_orig = X_train.copy()\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    lambdas = {}\n",
    "    cols_to_transform = []\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        col_skew = skew(X_train[col].dropna())\n",
    "        if (X_train[col] <= 0).any():\n",
    "            if plot:\n",
    "                print(f\"Skipping '{col}': contains non-positive values.\")\n",
    "            continue\n",
    "\n",
    "        if abs(col_skew) > skew_threshold:\n",
    "            try:\n",
    "                _, fitted_lambda = boxcox(X_train[col])\n",
    "                lambdas[col] = fitted_lambda\n",
    "                cols_to_transform.append(col)\n",
    "                if plot:\n",
    "                    print(f\"Applying Box-Cox to '{col}' (skew={col_skew:.2f}, λ={fitted_lambda:.3f})\")\n",
    "            except Exception as e:\n",
    "                if plot:\n",
    "                    print(f\"Failed to apply Box-Cox to '{col}': {e}\")\n",
    "                continue\n",
    "\n",
    "    if not cols_to_transform:\n",
    "        if plot:\n",
    "            print(\"No columns transformed with Box-Cox.\")\n",
    "        return X_train, X_val, X_test\n",
    "\n",
    "    def _apply_boxcox_with_lambda(df, col, lam):\n",
    "        df = df.copy()\n",
    "        if abs(lam) < 1e-6:\n",
    "            df[col] = np.log(df[col])\n",
    "        else:\n",
    "            df[col] = (np.power(df[col], lam) - 1) / lam\n",
    "        return df\n",
    "\n",
    "    for col in cols_to_transform:\n",
    "        lam = lambdas[col]\n",
    "        X_train = _apply_boxcox_with_lambda(X_train, col, lam)\n",
    "        X_val = _apply_boxcox_with_lambda(X_val, col, lam)\n",
    "        X_test = _apply_boxcox_with_lambda(X_test, col, lam)\n",
    "\n",
    "    if plot:\n",
    "        for col in cols_to_transform:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            sns.histplot(X_train_orig[col].dropna(), kde=True, ax=axes[0], color='skyblue')\n",
    "            axes[0].set_title(f'Original: {col}\\nSkew = {skew(X_train_orig[col].dropna()):.2f}')\n",
    "\n",
    "            sns.histplot(X_train[col].dropna(), kde=True, ax=axes[1], color='lightgreen')\n",
    "            axes[1].set_title(f'Box-Cox: {col}\\nSkew = {skew(X_train[col].dropna()):.2f}')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8708ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, X_test = apply_boxcox_to_all(\n",
    "#     X_train, X_val, X_test,\n",
    "#     skew_threshold=1.0,\n",
    "#     plot=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606cef4",
   "metadata": {},
   "source": [
    "#### 6.5.4 Yeo Johson Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "skewed_cols = []\n",
    "for col in numeric_columns:\n",
    "    if abs(skew(X_train[col].dropna())) > 1.0:\n",
    "        skewed_cols.append(col)\n",
    "\n",
    "print(\"Skewed columns to transform:\", skewed_cols)\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[skewed_cols] = pt.fit_transform(X_train[skewed_cols])\n",
    "# X_val[skewed_cols] = pt.transform(X_val[skewed_cols])\n",
    "# X_test[skewed_cols] = pt.transform(X_test[skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfa28b",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection & Treatment\n",
    "\n",
    "Implement IQR-based and Z-score methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e556e",
   "metadata": {},
   "source": [
    "### 7.1 Boxplot to check outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, num_cols):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(6,2))\n",
    "        sns.boxplot(x=df[col].dropna())\n",
    "        plt.title(f'Boxplot: {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_boxplots(X_train, numeric_columns)\n",
    "# plot_boxplots(X_val, numeric_columns)\n",
    "# plot_boxplots(X_test, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703847b",
   "metadata": {},
   "source": [
    "### 7.2 IQR Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, num_cols, k=1.5, verbose=True):\n",
    "    df = df.copy()\n",
    "    for c in num_cols:\n",
    "        Q1 = df[c].quantile(0.25)\n",
    "        Q3 = df[c].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - k * IQR\n",
    "        upper = Q3 + k * IQR\n",
    "        before = len(df)\n",
    "        df = df[(df[c] >= lower) & (df[c] <= upper)]\n",
    "        after = len(df)\n",
    "        if verbose:\n",
    "            print(f\"Column {c}: removed {before-after} rows using IQR (k={k})\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers_iqr(X_train, numeric_columns)\n",
    "# remove_outliers_iqr(X_val, numeric_columns)\n",
    "# remove_outliers_iqr(X_test, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369b063",
   "metadata": {},
   "source": [
    "### 7.3 Z-Score Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def remove_outliers_zscore(df, num_cols, z_thresh=3.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Remove rows with outliers in specified numerical columns using Z-score.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - num_cols: str or list of column names (e.g., 'age' or ['age', 'income'])\n",
    "    - z_thresh: Z-score threshold (default: 3.0)\n",
    "    - verbose: Print number of removed rows (default: True)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with outliers removed\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if isinstance(num_cols, str):\n",
    "        num_cols = [num_cols]\n",
    "\n",
    "    z_scores = np.abs(stats.zscore(df[num_cols], nan_policy='omit'))\n",
    "    mask = (z_scores < z_thresh).all(axis=1)\n",
    "    df_clean = df[mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Removed {len(df) - len(df_clean)} rows using Z-score threshold {z_thresh} on columns: {num_cols}\")\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = remove_outliers_zscore(X_train, numeric_columns)\n",
    "# X_val = remove_outliers_zscore(X_test, numeric_columns)\n",
    "# X_test = remove_outliers_zscore(X_train, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97787e85",
   "metadata": {},
   "source": [
    "### 7.4 Handling with Robust Scaler\n",
    "\n",
    "It is useful when the data contains outliers that cannot or should not be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "numeric_columns = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = scaler.transform(X_val[numeric_columns])\n",
    "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366d4bc",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering\n",
    "\n",
    "Examples: ratio features, date extraction, interaction terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15153d94",
   "metadata": {},
   "source": [
    "### 8.1 Ratio Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc823d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ratio_feature(df, numerator, denominator, new_name=None):\n",
    "    df = df.copy()\n",
    "    new_name = new_name or f\"{numerator}_over_{denominator}\"\n",
    "    df[new_name] = df[numerator] / (df[denominator].replace(0, np.nan) + 1e-9)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = add_ratio_feature(df, 'feature1', 'feature2', new_name='feature_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce26b30",
   "metadata": {},
   "source": [
    "### 8.2 Date Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df, date_cols):\n",
    "    \"\"\"Extract time-based features from datetime index.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in date_cols:\n",
    "        df['year'] = df[col].dt.year\n",
    "        df['month'] = df[col].dt.month\n",
    "        df['day'] = df[col].dt.day\n",
    "        df['hour'] = df[col].dt.hour\n",
    "        df['weekday'] = df[col].dt.weekday\n",
    "        df['is_weekend'] = df[col].dt.weekday >= 5\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_cols = ['timestamp', 'date', 'created_at']\n",
    "# X_train = extract_time_features(X_train, date_cols)\n",
    "# X_val = extract_time_features(X_val, date_cols)\n",
    "# X_test = extract_time_features(X_test, date_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63f2c4",
   "metadata": {},
   "source": [
    "## 9. Feature Selection\n",
    "\n",
    "Selecting the most important features improves model performance and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e053160",
   "metadata": {},
   "source": [
    "### 9.1 SelectKBest (Univariate Selection)\n",
    "\n",
    "Selects top K features based on statistical tests.\n",
    "\n",
    "- chi2 → for non-negative features (e.g., counts, frequencies).\n",
    "- f_classif → for continuous numerical features in classification problems.\n",
    "- Useful for quick filtering before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "k = 10\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_val_selected = selector.transform(X_val)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf53886",
   "metadata": {},
   "source": [
    "### 9.2 Recursive Feature Elimination (RFE)\n",
    "\n",
    "Iteratively trains a model and removes the least important features.\n",
    "\n",
    "- More computationally expensive.\n",
    "- Works best when you have a moderate number of features (< 100).\n",
    "- Can be used with any estimator that exposes a coef_ or feature_importances_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Apply RFE\n",
    "rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_features_rfe = X_train.columns[rfe.support_]\n",
    "print(\"Selected Features using RFE:\")\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd23a6",
   "metadata": {},
   "source": [
    "### 9.3 Feature Importance (Tree-based Models)\n",
    "\n",
    "Uses built-in feature importance scores from tree-based models (e.g., Random Forest, XGBoost).\n",
    "\n",
    "- Works only with tree-based models\n",
    "- Captures non-linear relationships.\n",
    "- Provides insights into feature relationships and importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7de9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "selected_features_rf = X.columns[indices[:10]]\n",
    "\n",
    "print(\"Top 10 Important Features (Random Forest):\")\n",
    "print(selected_features_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(X.columns[indices[:10]], importances[indices[:10]])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fd4e3",
   "metadata": {},
   "source": [
    "## 10. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e164b21",
   "metadata": {},
   "source": [
    "### 10.1 One-Hot Encoding\n",
    "Best for categorical features without ordinal relationship (e.g. color, city)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97275e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "color_encoded_train = encoder.fit_transform(X_train[['color']])\n",
    "color_encoded_val = encoder.transform(X_val[['color']])\n",
    "color_encoded_test = encoder.transform(X_test[['color']])\n",
    "\n",
    "# drop original categorical columns\n",
    "X_train = X_train.drop(columns=['color'])\n",
    "X_val = X_val.drop(columns=['color'])\n",
    "X_test = X_test.drop(columns=['color'])\n",
    "\n",
    "# concatenate the encoded features with the original dataframe\n",
    "X_train = X_train.join(pd.DataFrame(color_encoded_train, columns=encoder.get_feature_names_out(['color'])))\n",
    "X_val = X_val.join(pd.DataFrame(color_encoded_val, columns=encoder.get_feature_names_out(['color'])))\n",
    "X_test = X_test.join(pd.DataFrame(color_encoded_test, columns=encoder.get_feature_names_out(['color'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23b2ad",
   "metadata": {},
   "source": [
    "### 10.2 Ordinal Encoding\n",
    "Best for ordinal categorical features (e.g. education level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ba714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder(categories=[['High School','Bachelor','Master','PhD']])\n",
    "X_train['education_encoded'] = encoder.fit_transform(X_train[['education']])\n",
    "X_train.drop('education', axis=1, inplace=True)\n",
    "\n",
    "X_val['education_encoded'] = encoder.transform(X_val[['education']])\n",
    "X_val.drop('education', axis=1, inplace=True)\n",
    "\n",
    "X_test['education_encoded'] = encoder.transform(X_test[['education']])\n",
    "X_test.drop('education', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17a871",
   "metadata": {},
   "source": [
    "### 10.3 Label Encoding\n",
    "Best for labeling column that are categorical(y_train, y_val and y _test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc728d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 1. fit on y_train\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# 2. safe function\n",
    "def safe_transform(le, y, unknown_value=-1):\n",
    "    mapping = {label: idx for idx, label in enumerate(le.classes_)}\n",
    "    return np.array([mapping.get(label, unknown_value) for label in y])\n",
    "\n",
    "# 3. Apply safe function on y_val\n",
    "y_val = safe_transform(le, y_val)\n",
    "\n",
    "# 4. (Optional) Check for unknown labels in validation\n",
    "unknown_in_val = set(y_val) - set(le.classes_)\n",
    "if unknown_in_val:\n",
    "    print(f\"کلاس‌های ناشناخته در validation: {unknown_in_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d3fea",
   "metadata": {},
   "source": [
    "### 10.4 Frequency Encoding\n",
    "\n",
    "Used for categorical columns with many unique values (high cardinality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d41677",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = X_train['category'].value_counts(normalize=True)\n",
    "X_train['category_freq_enc'] = X_train['category'].map(freq)\n",
    "X_train = X_train.drop(columns=['category'])\n",
    "\n",
    "X_val['category_freq_enc'] = X_val['category'].map(freq)\n",
    "X_val = X_val.drop(columns=['category'])\n",
    "\n",
    "X_test['category_freq_enc'] = X_test['category'].map(freq)\n",
    "X_test = X_test.drop(columns=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f4979f",
   "metadata": {},
   "source": [
    "### 10.5 MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc39cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def preprocess_col(df, col, sep):\n",
    "    return (\n",
    "        df[col]\n",
    "        .fillna('')\n",
    "        .astype(str)\n",
    "        .str.split(sep)\n",
    "        .apply(lambda x: [label.strip() for label in x if label.strip()])\n",
    "    )\n",
    "\n",
    "col = 'genres'\n",
    "sep = '|'\n",
    "prefix = 'genre'\n",
    "\n",
    "X_train[col] = preprocess_col(X_train, col, sep)\n",
    "X_val[col] = preprocess_col(X_val, col, sep)\n",
    "X_test[col] = preprocess_col(X_test, col, sep)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(X_train[col])\n",
    "\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    binarized = pd.DataFrame(\n",
    "        mlb.transform(df[col]),\n",
    "        columns=[f'{prefix}_{lbl}' for lbl in mlb.classes_],\n",
    "        index=df.index\n",
    "    )\n",
    "    df.drop(columns=[col], inplace=True)\n",
    "    df[binarized.columns] = binarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c60da",
   "metadata": {},
   "source": [
    "## 11. Numerical Feature Scaling\n",
    "\n",
    "Choose scaler depending on data distribution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebbf20",
   "metadata": {},
   "source": [
    "### 11.1 StandardScaler\n",
    "\n",
    "Useful when features follow a **Gaussian distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = scaler.transform(X_val[numeric_columns])\n",
    "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3391e6",
   "metadata": {},
   "source": [
    "### 11.2 MinMaxScaler\n",
    "\n",
    "Useful when features have **different scales** but known **min/max ranges**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e834d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train[\"name_of_num_col\"] = scaler.fit_transform(X_train[\"name_of_num_col\"])\n",
    "X_val[\"name_of_num_col\"] = scaler.transform(X_val[\"name_of_num_col\"])\n",
    "X_test[\"name_of_num_col\"] = scaler.transform(X_test[\"name_of_num_col\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef9e10",
   "metadata": {},
   "source": [
    "### 11.3 RobustScaler\n",
    "\n",
    "Useful for data with **outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ffb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train['name_of_num_col'] = scaler.fit_transform(X_train['name_of_num_col'])\n",
    "X_val['name_of_num_col'] = scaler.transform(X_val['name_of_num_col'])\n",
    "X_test['name_of_num_col'] = scaler.transform(X_test['name_of_num_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72993ecc",
   "metadata": {},
   "source": [
    "### 11.4 Log Transformation\n",
    "\n",
    "For **skewed data** to make it more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train['name_of_num_col'] = np.log1p(X_train['name_of_num_col'])\n",
    "X_val['name_of_num_col'] = np.log1p(X_val['name_of_num_col'])\n",
    "X_test['name_of_num_col'] = np.log1p(X_test['name_of_num_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f720b",
   "metadata": {},
   "source": [
    "## 12. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924b6bc",
   "metadata": {},
   "source": [
    "### 12.1 Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10315af7",
   "metadata": {},
   "source": [
    "- **Logistic Regression**  \n",
    "  Description: Linear model for binary or multiclass classification; fast, interpretable, assumes linear boundaries.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  model = LogisticRegression(random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Decision Tree**  \n",
    "  Description: Tree-based model capturing non-linear relationships; easy to visualize, prone to overfitting.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Random Forest**  \n",
    "  Description: Ensemble of decision trees (bagging); robust, reduces overfitting, good default choice.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Gradient Boosting (sklearn)**  \n",
    "  Description: Sequential tree boosting; high accuracy, needs tuning, captures complex patterns.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "  model = GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **XGBoost**  \n",
    "  Description: Optimized gradient boosting; fast, scalable, excels on tabular data, requires tuning.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from xgboost import XGBClassifier\n",
    "  model = XGBClassifier(random_state=42, n_estimators=100, eval_metric='logloss')\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Support Vector Machine (SVM)**  \n",
    "  Description: Effective in high-dimensional spaces with kernel trick for non-linear boundaries; needs scaling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.svm import SVC\n",
    "  model = SVC(kernel='rbf', C=1.0, probability=True)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**  \n",
    "  Description: Non-parametric, instance-based; simple, slow at prediction, sensitive to scaling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  model = KNeighborsClassifier(n_neighbors=5)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Naive Bayes (Gaussian)**  \n",
    "  Description: Probabilistic, assumes feature independence; fast, works well for text or small datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  model = GaussianNB()\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Multilayer Perceptron (MLP)**  \n",
    "  Description: Feedforward neural network for non-linear mappings; needs tuning and scaling, computationally intensive.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "  model = MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=300)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Recurrent Neural Network (RNN)**  \n",
    "  Description: Neural network for sequential data; captures temporal dependencies, suitable for time series or text.  \n",
    "  Usage (Keras):  \n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "  model = Sequential([\n",
    "      SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "  model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "  ```\n",
    "\n",
    "- **Long Short-Term Memory (LSTM)**  \n",
    "  Description: Advanced RNN variant; handles long-term dependencies, ideal for complex sequential data.  \n",
    "  Usage (Keras):  \n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import LSTM, Dense\n",
    "  model = Sequential([\n",
    "      LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'),\n",
    "      Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "  model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "  ```\n",
    "\n",
    "- **Linear Regression / Ridge / Lasso** (Regression)  \n",
    "  Description: Linear models for continuous targets; Ridge/Lasso add regularization to prevent overfitting.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.linear_model import Ridge\n",
    "  model = Ridge(alpha=1.0)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```\n",
    "\n",
    "- **Support Vector Regressor (SVR)**  \n",
    "  Description: Non-linear regression with kernels; robust but sensitive to scaling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.svm import SVR\n",
    "  model = SVR(kernel='rbf', C=1.0)\n",
    "  model.fit(X_train, y_train)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387adcfb",
   "metadata": {},
   "source": [
    "### 12.2 Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77df8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **K-Means (Clustering)**  \n",
    "  Description: Partitions data into k clusters by centroid assignment; fast, assumes spherical clusters.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.cluster import KMeans\n",
    "  km = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "  labels = km.labels_\n",
    "  ```\n",
    "\n",
    "- **DBSCAN (Clustering)**  \n",
    "  Description: Density-based clustering; finds arbitrary-shaped clusters, handles noise, no need to specify k.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.cluster import DBSCAN\n",
    "  db = DBSCAN(eps=0.5, min_samples=5).fit(X)\n",
    "  labels = db.labels_\n",
    "  ```\n",
    "\n",
    "- **Isolation Forest (Anomaly Detection)**  \n",
    "  Description: Tree-based anomaly detection; isolates outliers by random splits, effective for high-dimensional data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.ensemble import IsolationForest\n",
    "  iso = IsolationForest(contamination=0.1, random_state=42).fit(X)\n",
    "  anomalies = iso.predict(X)  # -1 for outliers, 1 for inliers\n",
    "  ```\n",
    "\n",
    "- **Autoencoder (Dimensionality Reduction / Anomaly Detection)**  \n",
    "  Description: Neural network for unsupervised feature learning; compresses data or detects anomalies via reconstruction error.  \n",
    "  Usage (Keras):  \n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import Dense\n",
    "  model = Sequential([\n",
    "      Dense(32, activation='relu', input_shape=(X.shape[1],)),\n",
    "      Dense(16, activation='relu'),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dense(X.shape[1], activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='mse')\n",
    "  model.fit(X, X, epochs=10, batch_size=32)\n",
    "  ```\n",
    "\n",
    "- **PCA (Dimensionality Reduction)**  \n",
    "  Description: Linear projection to principal components; used for compression or visualization.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  from sklearn.decomposition import PCA\n",
    "  pca = PCA(n_components=2).fit(X)\n",
    "  X_reduced = pca.transform(X)\n",
    "  ```\n",
    "    ```\n",
    "\n",
    "Notes: choose models based on problem type (classification/regression), data size, feature scaling, interpretability needs, and compute budget. Always validate with cross-validation and tune hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18be9e",
   "metadata": {},
   "source": [
    "## 13.Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbe8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae043514",
   "metadata": {},
   "source": [
    "### 13.1 Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d3e1a",
   "metadata": {},
   "source": [
    "- **Mean Squared Error (MSE)**  \n",
    "  Description: Average of squared differences between predictions and actual values; penalizes larger errors heavily.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  mse = mean_squared_error(y_true, y_pred)\n",
    "  print(f\"MSE: {mse:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**  \n",
    "  Description: Square root of MSE; interpretable in the same units as the target.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "  print(f\"RMSE: {rmse:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Mean Absolute Error (MAE)**  \n",
    "  Description: Average of absolute differences; less sensitive to outliers than MSE.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  print(f\"MAE: {mae:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **R² Score (Coefficient of Determination)**  \n",
    "  Description: Proportion of variance explained by the model; ranges from 0 to 1 (higher is better).  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  print(f\"R² Score: {r2:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Residual Plot**  \n",
    "  Description: Visualizes prediction errors (residuals) to assess model fit; ideally, residuals are randomly scattered around 0.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  residuals = y_true - y_pred\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "  plt.axhline(y=0, color='r', linestyle='--')\n",
    "  plt.xlabel('Predicted Values')\n",
    "  plt.ylabel('Residuals')\n",
    "  plt.title('Residual Plot')\n",
    "  plt.show()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da44d38",
   "metadata": {},
   "source": [
    "### 13.2 Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76869996",
   "metadata": {},
   "source": [
    "- **Accuracy**  \n",
    "  Description: Proportion of correct predictions; good for balanced datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  accuracy = accuracy_score(y_true, y_pred)\n",
    "  print(f\"Accuracy: {accuracy:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Precision**  \n",
    "  Description: Proportion of true positives among positive predictions; useful when false positives are costly.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  precision = precision_score(y_true, y_pred, average='weighted')  # Use 'binary' for binary classification\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Recall (Sensitivity)**  \n",
    "  Description: Proportion of true positives identified; critical when false negatives are costly.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  recall = recall_score(y_true, y_pred, average='weighted')  # Use 'binary' for binary classification\n",
    "  print(f\"Recall: {recall:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **F1 Score**  \n",
    "  Description: Harmonic mean of precision and recall; balances both for imbalanced datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  f1 = f1_score(y_true, y_pred, average='weighted')  # Use 'binary' for binary classification\n",
    "  print(f\"F1 Score: {f1:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **ROC-AUC Score**  \n",
    "  Description: Area under the ROC curve; measures model's ability to distinguish classes (binary/multiclass). Requires probability scores.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')  # Use y_pred_proba for probabilities\n",
    "  print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "  ```\n",
    "\n",
    "- **Confusion Matrix**  \n",
    "  Description: Table showing true vs. predicted class counts; helps visualize classification performance.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  plt.figure(figsize=(6, 4))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('True')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172027b",
   "metadata": {},
   "source": [
    "## 14. Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, TomekLinks, EditedNearestNeighbours\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab4171",
   "metadata": {},
   "source": [
    "### 15.1 Oversampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda4553",
   "metadata": {},
   "source": [
    "- **Random Oversampling**  \n",
    "  Description: Randomly duplicates samples from the minority class to balance the dataset; simple but risks overfitting.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  ros = RandomOverSampler(random_state=42)\n",
    "  X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "  print(\"Class distribution after Random Oversampling:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **SMOTE (Synthetic Minority Oversampling Technique)**  \n",
    "  Description: Generates synthetic samples for the minority class by interpolating between existing samples; reduces overfitting compared to random oversampling.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  smote = SMOTE(random_state=42)\n",
    "  X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "  print(\"Class distribution after SMOTE:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **ADASYN (Adaptive Synthetic Sampling)**  \n",
    "  Description: Similar to SMOTE but focuses synthetic samples on harder-to-classify regions (near class boundaries); good for complex datasets.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  adasyn = ADASYN(random_state=42)\n",
    "  X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "  print(\"Class distribution after ADASYN:\", pd.Series(y_resampled).value_counts())\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34c48c",
   "metadata": {},
   "source": [
    "### 15.2 Undersampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798354d",
   "metadata": {},
   "source": [
    "- **Random Undersampling**  \n",
    "  Description: Randomly removes samples from the majority class to balance the dataset; simple but may discard valuable data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  rus = RandomUnderSampler(random_state=42)\n",
    "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "  print(\"Class distribution after Random Undersampling:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **Cluster Centroids**  \n",
    "  Description: Replaces majority class samples with centroids of clusters; preserves structure but may oversimplify data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  cc = ClusterCentroids(random_state=42)\n",
    "  X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "  print(\"Class distribution after Cluster Centroids:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **Tomek Links**  \n",
    "  Description: Removes majority class samples that are closest to minority class samples (links); improves class separation.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  tl = TomekLinks()\n",
    "  X_resampled, y_resampled = tl.fit_resample(X, y)\n",
    "  print(\"Class distribution after Tomek Links:\", pd.Series(y_resampled).value_counts())\n",
    "  ```\n",
    "\n",
    "- **Edited Nearest Neighbors (ENN)**  \n",
    "  Description: Removes majority class samples misclassified by their k-nearest neighbors; cleans noisy data.  \n",
    "  Usage:  \n",
    "  ```python\n",
    "  enn = EditedNearestNeighbours()\n",
    "  X_resampled, y_resampled = enn.fit_resample(X, y)\n",
    "  print(\"Class distribution after ENN:\", pd.Series(y_resampled).value_counts())\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
